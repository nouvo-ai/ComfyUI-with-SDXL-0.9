{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaaaaaaaaa"
      },
      "source": [
        "Git clone the repo and install the requirements. (ignore the pip errors about protobuf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbbbbbbbbb",
        "outputId": "42080a99-9ac8-4877-e106-aaf43fd6a4de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-= Initial setup ComfyUI =-\n",
            "Cloning into 'ComfyUI'...\n",
            "remote: Enumerating objects: 5713, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 5713 (delta 16), reused 21 (delta 11), pack-reused 5682\u001b[K\n",
            "Receiving objects: 100% (5713/5713), 2.86 MiB | 9.05 MiB/s, done.\n",
            "Resolving deltas: 100% (3728/3728), done.\n",
            "/content/ComfyUI\n",
            "-= Updating ComfyUI =-\n",
            "Already up to date.\n",
            "-= Install dependencies =-\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118, https://download.pytorch.org/whl/cu117\n",
            "Collecting xformers!=0.0.18\n",
            "  Downloading xformers-0.0.20-cp310-cp310-manylinux2014_x86_64.whl (109.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.0.1+cu118)\n",
            "Collecting torchdiffeq (from -r requirements.txt (line 2))\n",
            "  Downloading torchdiffeq-0.2.3-py3-none-any.whl (31 kB)\n",
            "Collecting torchsde (from -r requirements.txt (line 3))\n",
            "  Downloading torchsde-0.2.5-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops (from -r requirements.txt (line 4))\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers>=4.25.1 (from -r requirements.txt (line 5))\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.0 (from -r requirements.txt (line 6))\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (3.8.4)\n",
            "Collecting accelerate (from -r requirements.txt (line 8))\n",
            "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (6.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (8.4.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (1.10.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers!=0.0.18) (1.22.4)\n",
            "Collecting pyre-extensions==0.0.29 (from xformers!=0.0.18)\n",
            "  Downloading pyre_extensions-0.0.29-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2.0.0)\n",
            "Collecting typing-inspect (from pyre-extensions==0.0.29->xformers!=0.0.18)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 1)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->-r requirements.txt (line 1)) (16.0.6)\n",
            "Collecting boltons>=20.2.1 (from torchsde->-r requirements.txt (line 3))\n",
            "  Downloading boltons-23.0.0-py2.py3-none-any.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trampoline>=0.1.2 (from torchsde->-r requirements.txt (line 3))\n",
            "  Downloading trampoline-0.1.2-py3-none-any.whl (5.2 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers>=4.25.1->-r requirements.txt (line 5))\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r requirements.txt (line 5)) (23.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r requirements.txt (line 5)) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.25.1->-r requirements.txt (line 5)) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=4.25.1->-r requirements.txt (line 5))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 7)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 7)) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 7)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 7)) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 7)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 7)) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->-r requirements.txt (line 7)) (1.3.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 8)) (5.9.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers>=4.25.1->-r requirements.txt (line 5)) (2023.6.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp->-r requirements.txt (line 7)) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->-r requirements.txt (line 5)) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.25.1->-r requirements.txt (line 5)) (2023.5.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 1)) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyre-extensions==0.0.29->xformers!=0.0.18)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: trampoline, tokenizers, safetensors, boltons, mypy-extensions, einops, typing-inspect, huggingface-hub, transformers, pyre-extensions, xformers, torchsde, torchdiffeq, accelerate\n",
            "Successfully installed accelerate-0.20.3 boltons-23.0.0 einops-0.6.1 huggingface-hub-0.16.4 mypy-extensions-1.0.0 pyre-extensions-0.0.29 safetensors-0.3.1 tokenizers-0.13.3 torchdiffeq-0.2.3 torchsde-0.2.5 trampoline-0.1.2 transformers-4.30.2 typing-inspect-0.9.0 xformers-0.0.20\n"
          ]
        }
      ],
      "source": [
        "#@title Environment Setup\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "OPTIONS = {}\n",
        "\n",
        "USE_GOOGLE_DRIVE = False  #@param {type:\"boolean\"}\n",
        "UPDATE_COMFY_UI = True  #@param {type:\"boolean\"}\n",
        "WORKSPACE = 'ComfyUI'\n",
        "OPTIONS['USE_GOOGLE_DRIVE'] = USE_GOOGLE_DRIVE\n",
        "OPTIONS['UPDATE_COMFY_UI'] = UPDATE_COMFY_UI\n",
        "\n",
        "if OPTIONS['USE_GOOGLE_DRIVE']:\n",
        "    !echo \"Mounting Google Drive...\"\n",
        "    %cd /\n",
        "\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    WORKSPACE = \"/content/drive/MyDrive/ComfyUI\"\n",
        "    %cd /content/drive/MyDrive\n",
        "\n",
        "![ ! -d $WORKSPACE ] && echo -= Initial setup ComfyUI =- && git clone https://github.com/comfyanonymous/ComfyUI\n",
        "%cd $WORKSPACE\n",
        "\n",
        "if OPTIONS['UPDATE_COMFY_UI']:\n",
        "  !echo -= Updating ComfyUI =-\n",
        "  !git pull\n",
        "\n",
        "!echo -= Install dependencies =-\n",
        "!pip install xformers!=0.0.18 -r requirements.txt --extra-index-url https://download.pytorch.org/whl/cu118 --extra-index-url https://download.pytorch.org/whl/cu117"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cccccccccc"
      },
      "source": [
        "Download some models/checkpoints/vae or custom comfyui nodes (uncomment the commands for the ones you want)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "dddddddddd"
      },
      "outputs": [],
      "source": [
        "# Checkpoints\n",
        "\n",
        "# SD1.5\n",
        "# !wget -c https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.ckpt -P ./models/checkpoints/\n",
        "\n",
        "# SD2\n",
        "#!wget -c https://huggingface.co/stabilityai/stable-diffusion-2-1-base/resolve/main/v2-1_512-ema-pruned.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/stabilityai/stable-diffusion-2-1/resolve/main/v2-1_768-ema-pruned.safetensors -P ./models/checkpoints/\n",
        "\n",
        "# Some SD1.5 anime style\n",
        "#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix2/AbyssOrangeMix2_hard.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix3/AOM3A1_orangemixs.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/Models/AbyssOrangeMix3/AOM3A3_orangemixs.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/Linaqruf/anything-v3.0/resolve/main/anything-v3-fp16-pruned.safetensors -P ./models/checkpoints/\n",
        "\n",
        "# SDXL0.9\n",
        "# !wget -c --user=username --password=password https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9/resolve/main/sd_xl_base_0.9.safetensors -P ./models/checkpoints/\n",
        "# !wget -c --user=username --password=password https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9/resolve/main/sd_xl_refiner_0.9.safetensors -P ./models/checkpoints/\n",
        "\n",
        "# Waifu Diffusion 1.5 (anime style SD2.x 768-v)\n",
        "#!wget -c https://huggingface.co/waifu-diffusion/wd-1-5-beta2/resolve/main/checkpoints/wd-1-5-beta2-fp16.safetensors -P ./models/checkpoints/\n",
        "\n",
        "\n",
        "# unCLIP models\n",
        "#!wget -c https://huggingface.co/comfyanonymous/illuminatiDiffusionV1_v11_unCLIP/resolve/main/illuminatiDiffusionV1_v11-unclip-h-fp16.safetensors -P ./models/checkpoints/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/wd-1.5-beta2_unCLIP/resolve/main/wd-1-5-beta2-aesthetic-unclip-h-fp16.safetensors -P ./models/checkpoints/\n",
        "\n",
        "\n",
        "# VAE\n",
        "# !wget -c https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors -P ./models/vae/\n",
        "#!wget -c https://huggingface.co/WarriorMama777/OrangeMixs/resolve/main/VAEs/orangemix.vae.pt -P ./models/vae/\n",
        "#!wget -c https://huggingface.co/hakurei/waifu-diffusion-v1-4/resolve/main/vae/kl-f8-anime2.ckpt -P ./models/vae/\n",
        "\n",
        "\n",
        "# Loras\n",
        "#!wget -c https://civitai.com/api/download/models/10350 -O ./models/loras/theovercomer8sContrastFix_sd21768.safetensors #theovercomer8sContrastFix SD2.x 768-v\n",
        "#!wget -c https://civitai.com/api/download/models/10638 -O ./models/loras/theovercomer8sContrastFix_sd15.safetensors #theovercomer8sContrastFix SD1.x\n",
        "\n",
        "\n",
        "# T2I-Adapter\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_depth_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_seg_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_sketch_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_keypose_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_openpose_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_color_sd14v1.pth -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_canny_sd14v1.pth -P ./models/controlnet/\n",
        "\n",
        "# T2I Styles Model\n",
        "#!wget -c https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_style_sd14v1.pth -P ./models/style_models/\n",
        "\n",
        "# CLIPVision model (needed for styles model)\n",
        "#!wget -c https://huggingface.co/openai/clip-vit-large-patch14/resolve/main/pytorch_model.bin -O ./models/clip_vision/clip_vit14.bin\n",
        "\n",
        "\n",
        "# ControlNet\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11e_sd15_ip2p_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11e_sd15_shuffle_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_canny_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11f1p_sd15_depth_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_inpaint_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_lineart_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_mlsd_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_normalbae_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_seg_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_softedge_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15s2_lineart_anime_fp16.safetensors -P ./models/controlnet/\n",
        "#!wget -c https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11u_sd15_tile_fp16.safetensors -P ./models/controlnet/\n",
        "\n",
        "\n",
        "# Controlnet Preprocessor nodes by Fannovel16\n",
        "#!cd custom_nodes && git clone https://github.com/Fannovel16/comfy_controlnet_preprocessors; cd comfy_controlnet_preprocessors && python install.py\n",
        "\n",
        "\n",
        "# GLIGEN\n",
        "#!wget -c https://huggingface.co/comfyanonymous/GLIGEN_pruned_safetensors/resolve/main/gligen_sd14_textbox_pruned_fp16.safetensors -P ./models/gligen/\n",
        "\n",
        "\n",
        "# ESRGAN upscale model\n",
        "#!wget -c https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth -P ./models/upscale_models/\n",
        "#!wget -c https://huggingface.co/sberbank-ai/Real-ESRGAN/resolve/main/RealESRGAN_x2.pth -P ./models/upscale_models/\n",
        "#!wget -c https://huggingface.co/sberbank-ai/Real-ESRGAN/resolve/main/RealESRGAN_x4.pth -P ./models/upscale_models/\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FRF4ob-DWCS"
      },
      "source": [
        "Download SDXL v0.9 from Hugging Face (Needed Hugging Face ID, PW, SDXL Repository Access)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSu5bVao9ImJ",
        "outputId": "4cb77ca2-44b6-49d2-c583-ebee00972c2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "huggingface 사용자 이름을 입력하세요:··········\n",
            "비밀번호를 입력하세요:··········\n",
            "./models/checkpoints/SDXL_Base.safetensors 설치 완료\n",
            "./models/checkpoints/SDXL_Refiner.safetensors 설치 완료\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import urllib.request\n",
        "\n",
        "username = getpass.getpass('huggingface 사용자 이름을 입력하세요:')\n",
        "password = getpass.getpass('비밀번호를 입력하세요:')\n",
        "urls = [\n",
        "    'https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9/resolve/main/sd_xl_base_0.9.safetensors',\n",
        "    'https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9/resolve/main/sd_xl_refiner_0.9.safetensors'\n",
        "]\n",
        "save_paths = [\n",
        "    './models/checkpoints/sd_xl_base_0.9.safetensors',\n",
        "    './models/checkpoints/sd_xl_refiner_0.9.safetensors'\n",
        "]\n",
        "\n",
        "def auth_download(url, save_path):\n",
        "  password_mgr = urllib.request.HTTPPasswordMgrWithDefaultRealm()\n",
        "  password_mgr.add_password(None, url, username, password)\n",
        "  auth_handler = urllib.request.HTTPBasicAuthHandler(password_mgr)\n",
        "  opener = urllib.request.build_opener(auth_handler)\n",
        "  urllib.request.install_opener(opener)\n",
        "  try:\n",
        "    urllib.request.urlretrieve(url, save_path)\n",
        "    print(save_path, '설치 완료')\n",
        "  except Exception as e:\n",
        "    print('오류가 발생했습니다:', e)\n",
        "\n",
        "for i in range(len(urls)):\n",
        "  auth_download(urls[i], save_paths[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkkkkkkkkkkkkk"
      },
      "source": [
        "### Run ComfyUI with localtunnel (Recommended Way)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjjjjjjjjjjjj",
        "outputId": "edec2681-7f36-4b94-e90c-36a1c547e783"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25h/tools/node/bin/lt -> /tools/node/lib/node_modules/localtunnel/bin/lt.js\n",
            "+ localtunnel@2.0.2\n",
            "updated 1 package in 1.006s\n",
            "Total VRAM 15102 MB, total RAM 12983 MB\n",
            "Enabling highvram mode because your GPU has more vram than your computer has ram. If you don't want this use: --normalvram\n",
            "xformers version: 0.0.20\n",
            "2023-07-12 16:51:41.686373: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Set vram state to: HIGH_VRAM\n",
            "Device: cuda:0 Tesla T4\n",
            "Using xformers cross attention\n",
            "\n",
            "ComfyUI finished loading, trying to launch localtunnel (if it gets stuck here localtunnel is having issues)\n",
            "\n",
            "The password/enpoint ip for localtunnel is: 34.71.198.46\n",
            "your url is: https://four-dogs-post.loca.lt\n",
            "got prompt\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is None and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1536, context_dim is 1280 and using 24 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is None and using 12 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 768, context_dim is 1280 and using 12 heads.\n",
            "v_prediction False\n",
            "adm 2560\n",
            "making attention of type 'vanilla-xformers' with 512 in_channels\n",
            "building MemoryEfficientAttnBlock with 512 in_channels...\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "making attention of type 'vanilla-xformers' with 512 in_channels\n",
            "building MemoryEfficientAttnBlock with 512 in_channels...\n",
            "missing {'cond_stage_model.clip_g.transformer.text_model.embeddings.position_ids'}\n",
            "left over keys: dict_keys(['denoiser.log_sigmas', 'denoiser.sigmas'])\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is None and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 1280, context_dim is 2048 and using 20 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is None and using 10 heads.\n",
            "Setting up MemoryEfficientCrossAttention. Query dim is 640, context_dim is 2048 and using 10 heads.\n",
            "v_prediction False\n",
            "adm 2816\n",
            "making attention of type 'vanilla-xformers' with 512 in_channels\n",
            "building MemoryEfficientAttnBlock with 512 in_channels...\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "making attention of type 'vanilla-xformers' with 512 in_channels\n",
            "building MemoryEfficientAttnBlock with 512 in_channels...\n",
            "missing {'cond_stage_model.clip_g.transformer.text_model.embeddings.position_ids'}\n",
            "left over keys: dict_keys(['denoiser.log_sigmas', 'denoiser.sigmas'])\n",
            "torch.Size([1, 1280]) 4096 4096 0 0 4096 4096\n",
            "torch.Size([1, 1280]) 4096 4096 0 0 4096 4096\n",
            "DDIM Sampler: 100% 13/13 [00:11<00:00,  1.16it/s]\n",
            "torch.Size([1, 1280]) 4096 4096 0 0 6.0\n",
            "torch.Size([1, 1280]) 4096 4096 0 0 1.0\n",
            "DDIM Sampler: 100% 7/7 [00:06<00:00,  1.12it/s]\n",
            "Prompt executed in 108.95 seconds\n",
            "\n",
            "Stopped server\n",
            "Exception ignored in atexit callback: <function wait_for_tokens at 0x7f510d0c8d30>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/dispatch.py\", line 210, in wait_for_tokens\n",
            "    @atexit.register\n",
            "KeyboardInterrupt: \n"
          ]
        }
      ],
      "source": [
        "!npm install -g localtunnel\n",
        "\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import socket\n",
        "import urllib.request\n",
        "\n",
        "def iframe_thread(port):\n",
        "  while True:\n",
        "      time.sleep(0.5)\n",
        "      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "      result = sock.connect_ex(('127.0.0.1', port))\n",
        "      if result == 0:\n",
        "        break\n",
        "      sock.close()\n",
        "  print(\"\\nComfyUI finished loading, trying to launch localtunnel (if it gets stuck here localtunnel is having issues)\\n\")\n",
        "\n",
        "  print(\"The password/enpoint ip for localtunnel is:\", urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))\n",
        "  p = subprocess.Popen([\"lt\", \"--port\", \"{}\".format(port)], stdout=subprocess.PIPE)\n",
        "  for line in p.stdout:\n",
        "    print(line.decode(), end='')\n",
        "\n",
        "\n",
        "threading.Thread(target=iframe_thread, daemon=True, args=(8188,)).start()\n",
        "\n",
        "!python main.py --dont-print-server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gggggggggg"
      },
      "source": [
        "### Run ComfyUI with colab iframe (use only in case the previous way with localtunnel doesn't work)\n",
        "\n",
        "You should see the ui appear in an iframe. If you get a 403 error, it's your firefox settings or an extension that's messing things up.\n",
        "\n",
        "If you want to open it in another window use the link.\n",
        "\n",
        "Note that some UI features like live image previews won't work because the colab iframe blocks websockets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhhhhhhhhh"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "import time\n",
        "import socket\n",
        "def iframe_thread(port):\n",
        "  while True:\n",
        "      time.sleep(0.5)\n",
        "      sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
        "      result = sock.connect_ex(('127.0.0.1', port))\n",
        "      if result == 0:\n",
        "        break\n",
        "      sock.close()\n",
        "  from google.colab import output\n",
        "  output.serve_kernel_port_as_iframe(port, height=1024)\n",
        "  print(\"to open it in a window you can open this link here:\")\n",
        "  output.serve_kernel_port_as_window(port)\n",
        "\n",
        "threading.Thread(target=iframe_thread, daemon=True, args=(8188,)).start()\n",
        "\n",
        "!python main.py --dont-print-server"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
